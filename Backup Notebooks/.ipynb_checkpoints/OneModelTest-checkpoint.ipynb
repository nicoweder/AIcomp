{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf45e631-67c5-4f01-a704-326126889246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from event_detection_ap import score, ParticipantVisibleError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5db770-ef45-482e-ae5d-858daa62a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"processed/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8d11eb-736d-4869-aefe-d784bcb388a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    # === 1. Convert timestamp to datetime and remove timezone ===\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n",
    "    \n",
    "    # === 2. Hour-based features ===\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype('int8')\n",
    "    df[\"minute\"] = df[\"timestamp\"].dt.minute.astype('int8')\n",
    "    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek.astype('int8')\n",
    "    df[\"elapsed_time_from_midnight\"] = (df[\"hour\"] * 60 + df[\"minute\"]).astype('int32')\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype('int8')\n",
    "    df[\"is_night\"] = ((df[\"hour\"] >= 22) | (df[\"hour\"] <= 6)).astype('int8')\n",
    "\n",
    "    # === 3. Time-based features ===\n",
    "    periods = 20\n",
    "    df[\"anglez\"] = abs(df[\"anglez\"])\n",
    "    df[\"anglez_diff\"] = df.groupby('series_id')['anglez'].diff(periods=periods).bfill().astype('float16')\n",
    "    df[\"enmo_diff\"] = df.groupby('series_id')['enmo'].diff(periods=periods).bfill().astype('float16')\n",
    "\n",
    "    # === 4. Rolling statistical features (mean, max, std, etc.) ===\n",
    "    window_sizes = [12, 100, 360]  # in 5-second steps\n",
    "    for window in window_sizes:\n",
    "        for col in ['anglez', 'enmo']:\n",
    "            df[f'{col}_mean_{window}s'] = df[col].rolling(window, min_periods=1).mean().astype('float16')\n",
    "            df[f'{col}_std_{window}s'] = df[col].rolling(window, min_periods=1).std().astype('float16')\n",
    "            df[f'{col}_min_{window}s'] = df[col].rolling(window, min_periods=1).min().astype('float16')\n",
    "            df[f'{col}_max_{window}s'] = df[col].rolling(window, min_periods=1).max().astype('float16')\n",
    "            df[f'{col}_median_{window}s'] = df[col].rolling(window, min_periods=1).median().astype('float16')\n",
    "            df[f'{col}_cumulative_{window}s'] = df[col].rolling(window, min_periods=1).sum().astype('float16')\n",
    "\n",
    "    # === 5. Advanced Feature Engineering ===\n",
    "    df[\"anglez_delta\"] = (df[\"anglez\"] - df[\"anglez\"].shift(1)).astype('float16')\n",
    "\n",
    "    # Simple Moving Average (SMA) for enmo\n",
    "    sma_windows = [12, 100, 360]\n",
    "    for window in sma_windows:\n",
    "        df[f'enmo_sma_{window}s'] = df['enmo'].rolling(window, min_periods=1).mean().astype('float16')\n",
    "\n",
    "    # === 6. Lag Features ===\n",
    "    lag_targets = [\n",
    "        \"anglez\", \"enmo\",\n",
    "        \"anglez_mean_12s\", \"anglez_std_12s\",\n",
    "        \"enmo_mean_12s\", \"enmo_std_12s\",\n",
    "        \"anglez_mean_100s\", \"anglez_std_100s\",\n",
    "        \"enmo_mean_100s\", \"enmo_std_100s\"\n",
    "    ]\n",
    "    lag_steps = [1, 2, 3]\n",
    "    for col in lag_targets:\n",
    "        for lag in lag_steps:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag).astype('float16')\n",
    "\n",
    "    # === 7. Handle NaN values (filling missing values where appropriate) ===\n",
    "    # Separate numeric columns for NaN filling\n",
    "    # === 7. Handle NaN values (filling missing values where appropriate) ===\n",
    "    # Fill only numeric columns, converting float16 to float32 for compatibility\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].astype('float32').bfill().ffill().astype(df[numeric_cols].dtypes.to_dict())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b28025a8-a4a5-47f8-9757-0d10e2e5e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"hour\",\n",
    "    \"anglez\",\n",
    "    \"anglez_mean_100s\",\n",
    "    \"anglez_max_100s\",\n",
    "    \"anglez_std_100s\",\n",
    "    \"anglez_diff\",\n",
    "    \"enmo\",\n",
    "    \"enmo_mean_100s\",\n",
    "    \"enmo_max_100s\",\n",
    "    \"enmo_std_100s\",\n",
    "    \"enmo_diff\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe691e12-2d9a-4f06-9651-34e5ad0508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showcor(X):\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(X.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "    plt.title(\"Feature Correlation Heatmap (with values)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805ff900-6251-4e1f-81f7-60d387d444f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_random_forest(train_df, features, n_splits=3, model_params=None):\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'n_estimators': 100,\n",
    "            'min_samples_leaf': 300,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "    X = train_df[features]\n",
    "    y = train_df[\"awake\"]\n",
    "    groups = train_df[\"series_id\"]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    oof_preds_not_awake = np.zeros(len(X))\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "        print(f\"\\nüîÅ Fold {fold + 1}\")\n",
    "        X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = RandomForestClassifier(**model_params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        probs = model.predict_proba(X_val)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        oof_preds[val_idx] = probs[:, 1]\n",
    "        oof_preds_not_awake[val_idx] = probs[:, 0]\n",
    "\n",
    "        precision = precision_score(y_val, preds)\n",
    "        recall = recall_score(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds)\n",
    "\n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "\n",
    "        print(f\"üìà Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "        feature_importances += model.feature_importances_\n",
    "\n",
    "    avg_importances = feature_importances / n_splits\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': avg_importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    metrics_df = pd.DataFrame(fold_metrics)\n",
    "    print(\"\\nüìä Fold-wise metrics:\\n\", metrics_df)\n",
    "    print(f\"\\nüîç Mean F1 Score: {metrics_df['f1_score'].mean():.4f}\")\n",
    "    print(\"\\nüî• Feature Importances:\\n\", importance_df)\n",
    "\n",
    "    return oof_preds, oof_preds_not_awake, model, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b563fdf6-4e11-47fe-bd80-0ffcb0bbb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_predictions(df, awake_col=\"awake_pred\", not_awake_col=\"not_awake_pred\", smoothing_length=460):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"score\"] = df[awake_col].rolling(smoothing_length, center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "    df[\"smooth\"] = df[not_awake_col].rolling(smoothing_length, center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "    \n",
    "    # Re-binarize\n",
    "    df[\"smooth\"] = df[\"smooth\"].round()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd6a550d-1404-477a-9d1a-2c64e670001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to determine the onset and wakeup events\n",
    "def get_event(df):\n",
    "    lstCV = zip(df.series_id, df.smooth)\n",
    "    lstPOI = []\n",
    "    for (c, v), g in groupby(lstCV, lambda cv: (cv[0], cv[1] != 0 and not pd.isnull(cv[1]))):\n",
    "        llg = sum(1 for item in g)\n",
    "        if v is False:\n",
    "            lstPOI.extend([0] * llg)\n",
    "        else:\n",
    "            lstPOI.extend(['onset'] + (llg - 2) * [0] + ['wakeup'] if llg > 1 else [0])\n",
    "    return lstPOI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80373f34-203d-4ebb-8d14-1392a68908d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_filter(result_events, step_diff_min):\n",
    "    df = pd.DataFrame(result_events)\n",
    "    \n",
    "    # Set a threshold for valid pairings (step difference > 15000 is invalid)\n",
    "    step_diff_min_threshold = step_diff_min\n",
    "    \n",
    "    # Sort by series_id and step\n",
    "    df = df.sort_values(by=['series_id', 'step'])\n",
    "    \n",
    "    # Initialize a list to store valid event pairs\n",
    "    valid_pairs = []\n",
    "    \n",
    "    # Iterate through each unique series_id and match onsets and wakeups\n",
    "    for series_id in df['series_id'].unique():\n",
    "        series_data = df[df['series_id'] == series_id]\n",
    "        \n",
    "        onset_event = None\n",
    "        for index, row in series_data.iterrows():\n",
    "            if row['event'] == 'onset':\n",
    "                onset_event = row\n",
    "            elif row['event'] == 'wakeup' and onset_event is not None:\n",
    "                # Ensure the step difference between onset and wakeup is above the minimum threshold\n",
    "                step_diff = abs(row['step'] - onset_event['step'])\n",
    "                if step_diff >= step_diff_min_threshold:\n",
    "                    valid_pairs.append((onset_event, row))  # Add the valid pair\n",
    "                onset_event = None  # Reset for next pairing\n",
    "    \n",
    "    # Create a DataFrame for the valid pairs in the required format\n",
    "    output_data = []\n",
    "    for onset, wakeup in valid_pairs:\n",
    "        output_data.append({\n",
    "            'series_id': onset['series_id'],\n",
    "            'step': onset['step'],\n",
    "            'event': 'onset',\n",
    "            'score': onset['score']\n",
    "        })\n",
    "        output_data.append({\n",
    "            'series_id': wakeup['series_id'],\n",
    "            'step': wakeup['step'],\n",
    "            'event': 'wakeup',\n",
    "            'score': wakeup['score']\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the output data\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    output_df.to_csv('valid_pairs_predictions.csv', index=False)\n",
    "    \n",
    "    print(\"Results saved to 'valid_pairs_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef9fdb32-8c36-46c1-a9c8-8e8529ccffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_pipeline():\n",
    "\n",
    "\n",
    "    # Load ground truth and predictions\n",
    "    solution = pd.read_csv('processed/event_cleaned_final.csv')             # Ground truth\n",
    "    submission = pd.read_csv('valid_pairs_predictions.csv')                      # Your predictions from train set\n",
    "    \n",
    "    # Define tolerances\n",
    "    tolerances = {\n",
    "        \"onset\":  [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "        \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    }\n",
    "    \n",
    "    # Set correct column names used in your prediction file\n",
    "    column_names = {\n",
    "        'series_id_column_name': 'series_id',\n",
    "        'time_column_name': 'step',\n",
    "        'event_column_name': 'event',\n",
    "        'score_column_name': 'score',  # You named the prediction confidence 'score'\n",
    "    }\n",
    "    \n",
    "    # Run scoring\n",
    "    try:\n",
    "        ap_score = score(solution, submission, tolerances, **column_names)\n",
    "        print(f\"\\n‚úÖ Average Precision Score: {ap_score}\")\n",
    "    except ParticipantVisibleError as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "\n",
    "\n",
    "    return ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6c1a080-0f1a-4ee5-b226-5a2c162d5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names, top_n=None, save_path=None, title=\"Feature Importances\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    feat_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    }).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    if top_n:\n",
    "        feat_df = feat_df.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_df[\"feature\"], feat_df[\"importance\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_path:\n",
    "        feat_df.to_csv(save_path, index=False)\n",
    "        print(f\"üìÅ Feature importances saved to: {save_path}\")\n",
    "    \n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaf7ff6d-1516-4795-b72d-5bc59356a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(train, features, step_diff_min=2000, n_splits=3, model_params=None):\n",
    "    print(\"üöÄ Starting full pipeline...\")\n",
    "\n",
    "    # Step 1: Feature generation\n",
    "    train_df = make_features(train)\n",
    "    showcor(train_df)\n",
    "    # Step 2: Train model and get predictions\n",
    "    oof_preds, oof_preds_not_awake, last_model, feature_names = run_cv_random_forest(\n",
    "        train_df, features, n_splits=n_splits, model_params=model_params\n",
    "    )\n",
    "    plot_feature_importances(last_model, feature_names, top_n=15, save_path=\"feature_importances.csv\")\n",
    "    # Step 3: Apply predictions and smoothing\n",
    "    train_df[\"awake_pred\"] = oof_preds\n",
    "    train_df[\"not_awake_pred\"] = oof_preds_not_awake\n",
    "    train_df = smooth_predictions(train_df)\n",
    "\n",
    "    # Step 4: Detect events\n",
    "    train_df[\"event\"] = get_event(train_df)\n",
    "    train_events = train_df.loc[train_df[\"event\"] != 0, [\"series_id\", \"step\", \"event\", \"score\"]].reset_index(drop=True)\n",
    "    train_events.to_csv('result_events.csv', index=False)\n",
    "    print(\"\\n‚úÖ Events saved to: result_events.csv\")\n",
    "\n",
    "    # Step 5: Filter onset-wakeup pairs\n",
    "    apply_filter(train_events, step_diff_min=step_diff_min)\n",
    "\n",
    "    # Step 6: Evaluate\n",
    "    ap_score = evaluate_full_pipeline()\n",
    "    \n",
    "    return ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3007348-86c3-481b-9029-2e55f7fbb1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full pipeline...\n",
      "\n",
      "üîÅ Fold 1\n",
      "üìà Precision: 0.9757 | Recall: 0.9688 | F1: 0.9722\n",
      "\n",
      "üîÅ Fold 2\n",
      "üìà Precision: 0.9737 | Recall: 0.9521 | F1: 0.9628\n",
      "\n",
      "üîÅ Fold 3\n",
      "üìà Precision: 0.9797 | Recall: 0.9672 | F1: 0.9734\n",
      "\n",
      "üìä Fold-wise metrics:\n",
      "    fold  precision    recall  f1_score\n",
      "0     1   0.975725  0.968763  0.972232\n",
      "1     2   0.973714  0.952150  0.962811\n",
      "2     3   0.979729  0.967210  0.973430\n",
      "\n",
      "üîç Mean F1 Score: 0.9695\n",
      "\n",
      "üî• Feature Importances:\n",
      "              feature  importance\n",
      "9      enmo_std_100s    0.256022\n",
      "0               hour    0.247109\n",
      "8      enmo_max_100s    0.185582\n",
      "7     enmo_mean_100s    0.142484\n",
      "4    anglez_std_100s    0.064103\n",
      "6               enmo    0.036779\n",
      "10         enmo_diff    0.019349\n",
      "2   anglez_mean_100s    0.017899\n",
      "5        anglez_diff    0.016000\n",
      "1             anglez    0.007462\n",
      "3    anglez_max_100s    0.007211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13945/1070954544.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[\"score\"] = df[awake_col].rolling(smoothing_length, center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
      "/tmp/ipykernel_13945/1070954544.py:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[\"smooth\"] = df[not_awake_col].rolling(smoothing_length, center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Events saved to: result_events.csv\n",
      "Results saved to 'valid_pairs_predictions.csv'.\n",
      "\n",
      "‚úÖ Average Precision Score: 0.41900799586462145\n"
     ]
    }
   ],
   "source": [
    "ap = main_pipeline(train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15939d51-0b39-42cf-8ce3-09df9c92df1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
