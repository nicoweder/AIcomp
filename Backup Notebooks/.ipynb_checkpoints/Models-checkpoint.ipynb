{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2826930-cd2c-43f0-b5a6-4425a0d090c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from xgboost) (1.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e59f963-a253-4bb4-95e0-7b9ccdeb4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from event_detection_ap import score, ParticipantVisibleError\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a615f3fb-a079-49e1-a7fd-cac1ffdd4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_SPLITS = 5\n",
    "THRESHOLD = 0.05\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"processed/merged_dff_gold84_V3.parquet\"\n",
    "CANDIDATES_PATH = \"results/model1_candidates_kfold.csv\"\n",
    "EVENTS_PATH = \"processed/event_cleaned.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111dde1a-439e-440f-ab1c-6f99e27bcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_windowing(df):\n",
    "    df[\"step_int\"] = df[\"step\"].astype(int)\n",
    "    df[\"step_offset\"] = df.groupby(\"series_id\")[\"step_int\"].transform(lambda x: x - x.min())\n",
    "    return df[df[\"step_offset\"] % 3 == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4857f485-0ae5-458a-96a8-d5fa52ccc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_labels(df, radius=4):\n",
    "    target_array = df[\"target\"].values.copy()\n",
    "    for shift in range(1, radius + 1):\n",
    "        df[f\"target_shift_minus_{shift}\"] = df[\"target\"].shift(-shift, fill_value=0)\n",
    "        df[f\"target_shift_plus_{shift}\"] = df[\"target\"].shift(shift, fill_value=0)\n",
    "        target_array |= df[f\"target_shift_minus_{shift}\"].values\n",
    "        target_array |= df[f\"target_shift_plus_{shift}\"].values\n",
    "    df[\"target\"] = target_array\n",
    "    return df.drop(columns=[c for c in df.columns if c.startswith(\"target_shift_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64eb9aee-5abf-4c43-94aa-8ab6d4c5a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Feature Set\n",
    "final_preset = [\n",
    "    \"anglez\", \"enmo\", \"hour\", \"minute\", \"is_night\", \"is_weekend\",\n",
    "    \"anglez_delta\", \"anglez_lag_1\", \"enmo_lag_1\",\n",
    "    \"anglez_mean_60s\", \"enmo_mean_60s\",\n",
    "    \"enmo_cumulative_60s\", \"enmo_sma_60s\",\n",
    "    \"enmo_mean_12s_lag_1\", \"anglez_std_60s_lag_1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a776af43-2078-4578-a823-b01b4faf2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed shape: (8384569, 86)\n"
     ]
    }
   ],
   "source": [
    "df = candidate_windowing(df)\n",
    "df = expand_labels(df)\n",
    "print(f\"✅ Preprocessed shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08810a57-f00b-4883-bf9d-2411494edc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8384569 entries, 0 to 8384568\n",
      "Data columns (total 86 columns):\n",
      " #   Column                      Dtype              \n",
      "---  ------                      -----              \n",
      " 0   series_id                   object             \n",
      " 1   step                        uint32             \n",
      " 2   timestamp                   datetime64[us, UTC]\n",
      " 3   anglez                      float32            \n",
      " 4   enmo                        float32            \n",
      " 5   night                       float64            \n",
      " 6   event                       int64              \n",
      " 7   hour                        int8               \n",
      " 8   minute                      int8               \n",
      " 9   day_of_week                 int8               \n",
      " 10  elapsed_time_from_midnight  int32              \n",
      " 11  is_weekend                  int8               \n",
      " 12  anglez_mean_12s             float32            \n",
      " 13  anglez_std_12s              float32            \n",
      " 14  anglez_min_12s              float32            \n",
      " 15  anglez_max_12s              float32            \n",
      " 16  anglez_median_12s           float32            \n",
      " 17  anglez_cumulative_12s       float32            \n",
      " 18  enmo_mean_12s               float32            \n",
      " 19  enmo_std_12s                float32            \n",
      " 20  enmo_min_12s                float32            \n",
      " 21  enmo_max_12s                float32            \n",
      " 22  enmo_median_12s             float32            \n",
      " 23  enmo_cumulative_12s         float32            \n",
      " 24  anglez_mean_60s             float32            \n",
      " 25  anglez_std_60s              float32            \n",
      " 26  anglez_min_60s              float32            \n",
      " 27  anglez_max_60s              float32            \n",
      " 28  anglez_median_60s           float32            \n",
      " 29  anglez_cumulative_60s       float32            \n",
      " 30  enmo_mean_60s               float32            \n",
      " 31  enmo_std_60s                float32            \n",
      " 32  enmo_min_60s                float32            \n",
      " 33  enmo_max_60s                float32            \n",
      " 34  enmo_median_60s             float32            \n",
      " 35  enmo_cumulative_60s         float32            \n",
      " 36  anglez_mean_360s            float32            \n",
      " 37  anglez_std_360s             float32            \n",
      " 38  anglez_min_360s             float32            \n",
      " 39  anglez_max_360s             float32            \n",
      " 40  anglez_median_360s          float32            \n",
      " 41  anglez_cumulative_360s      float32            \n",
      " 42  enmo_mean_360s              float32            \n",
      " 43  enmo_std_360s               float32            \n",
      " 44  enmo_min_360s               float32            \n",
      " 45  enmo_max_360s               float32            \n",
      " 46  enmo_median_360s            float32            \n",
      " 47  enmo_cumulative_360s        float32            \n",
      " 48  is_night                    int8               \n",
      " 49  anglez_delta                float32            \n",
      " 50  enmo_sma_12s                float32            \n",
      " 51  enmo_sma_60s                float32            \n",
      " 52  enmo_sma_360s               float32            \n",
      " 53  anglez_lag_1                float32            \n",
      " 54  anglez_lag_2                float32            \n",
      " 55  anglez_lag_3                float32            \n",
      " 56  enmo_lag_1                  float32            \n",
      " 57  enmo_lag_2                  float32            \n",
      " 58  enmo_lag_3                  float32            \n",
      " 59  anglez_mean_12s_lag_1       float32            \n",
      " 60  anglez_mean_12s_lag_2       float32            \n",
      " 61  anglez_mean_12s_lag_3       float32            \n",
      " 62  anglez_std_12s_lag_1        float32            \n",
      " 63  anglez_std_12s_lag_2        float32            \n",
      " 64  anglez_std_12s_lag_3        float32            \n",
      " 65  enmo_mean_12s_lag_1         float32            \n",
      " 66  enmo_mean_12s_lag_2         float32            \n",
      " 67  enmo_mean_12s_lag_3         float32            \n",
      " 68  enmo_std_12s_lag_1          float32            \n",
      " 69  enmo_std_12s_lag_2          float32            \n",
      " 70  enmo_std_12s_lag_3          float32            \n",
      " 71  anglez_mean_60s_lag_1       float32            \n",
      " 72  anglez_mean_60s_lag_2       float32            \n",
      " 73  anglez_mean_60s_lag_3       float32            \n",
      " 74  anglez_std_60s_lag_1        float32            \n",
      " 75  anglez_std_60s_lag_2        float32            \n",
      " 76  anglez_std_60s_lag_3        float32            \n",
      " 77  enmo_mean_60s_lag_1         float32            \n",
      " 78  enmo_mean_60s_lag_2         float32            \n",
      " 79  enmo_mean_60s_lag_3         float32            \n",
      " 80  enmo_std_60s_lag_1          float32            \n",
      " 81  enmo_std_60s_lag_2          float32            \n",
      " 82  enmo_std_60s_lag_3          float32            \n",
      " 83  target                      int64              \n",
      " 84  step_int                    int64              \n",
      " 85  step_offset                 int64              \n",
      "dtypes: datetime64[us, UTC](1), float32(72), float64(1), int32(1), int64(4), int8(5), object(1), uint32(1)\n",
      "memory usage: 2.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece86f51-6b10-4b56-b9b7-13473fa948c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess_predictions(y_probs, threshold):\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    event_indices = np.where(y_pred == 1)[0]\n",
    "    filtered = []\n",
    "    last_idx = -np.inf\n",
    "    for idx in event_indices:\n",
    "        if idx - last_idx >= 12:\n",
    "            filtered.append(idx)\n",
    "            last_idx = idx\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0804aa6-aafe-4320-9ef9-2549838cb96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:45:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model for fold 1 to Safed Models/xgb_model_fold2_1.joblib\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:46:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model for fold 2 to Safed Models/xgb_model_fold2_2.joblib\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:46:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model for fold 3 to Safed Models/xgb_model_fold2_3.joblib\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:46:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model for fold 4 to Safed Models/xgb_model_fold2_4.joblib\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:46:45] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model for fold 5 to Safed Models/xgb_model_fold2_5.joblib\n",
      "✅ Saved 10871 candidates to results/model1_candidates_kfold.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)  # Create group-based cross-validation splits\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, df[\"target\"], groups=df[\"series_id\"])):\n",
    "    print(f\"Fold {fold + 1}/{N_SPLITS}\")\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train = df.iloc[train_idx][final_preset].astype(np.float32)\n",
    "    y_train = df.iloc[train_idx][\"target\"]\n",
    "    X_val = df.iloc[val_idx][final_preset].astype(np.float32)\n",
    "    y_val = df.iloc[val_idx][\"target\"]\n",
    "    meta_val = df.iloc[val_idx][[\"series_id\", \"step\"]].reset_index(drop=True)\n",
    "\n",
    "    # Initialize and train XGBoost model\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model1.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities and postprocess\n",
    "    y_probs = model1.predict_proba(X_val)[:, 1]\n",
    "    event_idxs = postprocess_predictions(y_probs, THRESHOLD)\n",
    "\n",
    "    # Store results for current fold\n",
    "    fold_results = meta_val.loc[event_idxs].copy()\n",
    "    fold_results[\"model1_score\"] = y_probs[event_idxs]\n",
    "    results.append(fold_results)\n",
    "    \n",
    "    # Save model after each fold\n",
    "    model_filename = f'Safed Models/xgb_model_fold2_{fold + 1}.joblib'\n",
    "    joblib.dump(model1, model_filename)\n",
    "    print(f\"✅ Saved model for fold {fold + 1} to {model_filename}\")\n",
    "\n",
    "# Combine results from all folds\n",
    "candidates = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Save predictions to CSV\n",
    "candidates.to_csv(CANDIDATES_PATH, index=False)\n",
    "print(f\"✅ Saved {len(candidates)} candidates to {CANDIDATES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f822557-da8b-4d21-be45-fd3e3e280b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(candidates_df, true_events_df, step_tolerance=12):\n",
    "    \"\"\"\n",
    "    Compares predicted event candidates to ground-truth sleep events using a step tolerance.\n",
    "    \n",
    "    Args:\n",
    "        candidates_df (pd.DataFrame): Model predictions with 'series_id' and 'step'.\n",
    "        true_events_df (pd.DataFrame): True events with 'series_id' and 'step'.\n",
    "        step_tolerance (int): Max allowed difference in step between prediction and true event.\n",
    "        \n",
    "    Returns:\n",
    "        precision, recall, f1\n",
    "    \"\"\"\n",
    "    preds = candidates_df.copy()\n",
    "    trues = true_events_df.copy()\n",
    "    \n",
    "    preds[\"matched\"] = False\n",
    "    trues[\"matched\"] = False\n",
    "\n",
    "    for i, true_event in trues.iterrows():\n",
    "        series_id = true_event[\"series_id\"]\n",
    "        true_step = true_event[\"step\"]\n",
    "        \n",
    "        # Get predictions in same series and within step_tolerance\n",
    "        candidates_in_series = preds[\n",
    "            (preds[\"series_id\"] == series_id) &\n",
    "            (~preds[\"matched\"]) &\n",
    "            (np.abs(preds[\"step\"] - true_step) <= step_tolerance)\n",
    "        ]\n",
    "        \n",
    "        if not candidates_in_series.empty:\n",
    "            closest_idx = candidates_in_series.iloc[\n",
    "                np.argmin(np.abs(candidates_in_series[\"step\"] - true_step))\n",
    "            ].name\n",
    "            preds.at[closest_idx, \"matched\"] = True\n",
    "            trues.at[i, \"matched\"] = True\n",
    "\n",
    "    TP = preds[\"matched\"].sum()\n",
    "    FP = (~preds[\"matched\"]).sum()\n",
    "    FN = (~trues[\"matched\"]).sum()\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    print(f\"✅ Evaluation with step_tolerance = {step_tolerance}\")\n",
    "    print(f\"True Positives: {TP}\")\n",
    "    print(f\"False Positives: {FP}\")\n",
    "    print(f\"False Negatives: {FN}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6b2180-2074-47f9-896d-d57f1a134e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation with step_tolerance = 12\n",
      "True Positives: 569\n",
      "False Positives: 10302\n",
      "False Negatives: 2384\n",
      "Precision: 0.052\n",
      "Recall: 0.193\n",
      "F1 Score: 0.082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.05234109097599117),\n",
       " np.float64(0.19268540467321368),\n",
       " np.float64(0.08232060185185186))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load true events\n",
    "true_events_df = pd.read_csv(EVENTS_PATH)\n",
    "\n",
    "# Optionally filter to only relevant events and cast step to int\n",
    "true_events_df = true_events_df[[\"series_id\", \"step\"]].dropna()\n",
    "true_events_df[\"step\"] = true_events_df[\"step\"].astype(int)\n",
    "\n",
    "# Load candidate predictions\n",
    "candidates_df = pd.read_csv(CANDIDATES_PATH)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_predictions(candidates_df, true_events_df, step_tolerance=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40dee82f-0ddf-4bf0-a61b-9ee1fbae7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gefilterte Kandidaten gespeichert: 2528 Zeilen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12681/779148942.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = c.groupby('series_id').apply(select_max_score_in_range).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load previously saved candidates\n",
    "c = pd.read_csv(CANDIDATES_PATH)\n",
    "\n",
    "# Sort by 'series_id' and 'step'\n",
    "c = c.sort_values(by=[\"series_id\", \"step\"])\n",
    "\n",
    "# Function to select the highest 'model1_score' within a 4000-step range\n",
    "def select_max_score_in_range(group):\n",
    "    selected_rows = []  # Store filtered rows\n",
    "    \n",
    "    # Iterate over each row and keep only the best one in the step range\n",
    "    for i, row in group.iterrows():\n",
    "        # Define a 4000-step window around the current step\n",
    "        step_range = (row['step'] - 3600, row['step'] + 3500)\n",
    "        \n",
    "        # Filter rows within this range\n",
    "        range_group = group[(group['step'] >= step_range[0]) & (group['step'] <= step_range[1])]\n",
    "        \n",
    "        # Select the row with the highest model1_score\n",
    "        best_row = range_group.loc[range_group['model1_score'].idxmax()]\n",
    "        selected_rows.append(best_row)\n",
    "    \n",
    "    # Return selected rows as a DataFrame\n",
    "    return pd.DataFrame(selected_rows)\n",
    "\n",
    "# Apply filtering function to each series_id group\n",
    "result = c.groupby('series_id').apply(select_max_score_in_range).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicates to ensure one prediction per step\n",
    "result = result.drop_duplicates(subset=[\"step\"], keep=\"first\")\n",
    "\n",
    "# Save the filtered candidates\n",
    "result.to_csv('results/filtered_candidates.csv', index=False)\n",
    "\n",
    "print(f\"Filtered candidates saved: {result.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46e993f7-27c6-438f-a4d4-27aa9513f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Die Datei mit den berechneten Time Diff gespeichert: 2528 Zeilen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the already filtered candidates from the result file\n",
    "result = pd.read_csv('results/filtered_candidates.csv')\n",
    "\n",
    "# Sort the data by 'series_id' and 'step'\n",
    "result = result.sort_values(by=[\"series_id\", \"step\"])\n",
    "\n",
    "# Calculate the 'time_diff' for each 'series_id'\n",
    "result['time_diff'] = result.groupby('series_id')['step'].diff().fillna(result['step'])\n",
    "\n",
    "# Save the result with the new 'time_diff' column to a new CSV file\n",
    "result.to_csv('results/timediff.csv', index=False)\n",
    "\n",
    "print(f\"Saved the file with calculated time differences: {result.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c60f846-252a-4676-a3c8-79b45374f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Die Datei mit den alternierenden Labels gespeichert: 2528 Zeilen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12681/2624486615.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  timediff = timediff.groupby('series_id').apply(assign_alternating_labels)\n"
     ]
    }
   ],
   "source": [
    "# Load the file with calculated time differences\n",
    "timediff = pd.read_csv('results/timediff.csv')\n",
    "\n",
    "# Sort data by 'series_id' and 'step'\n",
    "timediff = timediff.sort_values(by=[\"series_id\", \"step\"])\n",
    "\n",
    "# Define a function to assign alternating labels (onset/wakeup)\n",
    "def assign_alternating_labels(group):\n",
    "    # Split into even and odd rows\n",
    "    even_rows = group.iloc[::2]\n",
    "    odd_rows = group.iloc[1::2]\n",
    "    \n",
    "    # Calculate average time_diff for even and odd rows\n",
    "    avg_time_even = even_rows['time_diff'].mean() if not even_rows.empty else 0\n",
    "    avg_time_odd = odd_rows['time_diff'].mean() if not odd_rows.empty else 0\n",
    "    \n",
    "    # Assign event labels based on which has the higher average time_diff\n",
    "    if avg_time_even > avg_time_odd:\n",
    "        group['event'] = ['onset' if i % 2 == 0 else 'wakeup' for i in range(len(group))]\n",
    "    else:\n",
    "        group['event'] = ['onset' if i % 2 != 0 else 'wakeup' for i in range(len(group))]\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the labeling function to each group of 'series_id'\n",
    "timediff = timediff.groupby('series_id').apply(assign_alternating_labels)\n",
    "\n",
    "# Save the result with assigned event labels\n",
    "timediff.to_csv('results/labeled_candidates.csv', index=False)\n",
    "\n",
    "print(f\"✅ Saved file with alternating labels: {timediff.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da32991f-99f0-4ee7-82b5-6d0a952a4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.024597218231661333\n"
     ]
    }
   ],
   "source": [
    "solution = pd.read_csv('processed/event_cleaned.csv')\n",
    "submission = pd.read_csv('results/labeled_candidates.csv')\n",
    "tolerances = {\n",
    "    \"onset\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "}\n",
    "\n",
    "column_names = {\n",
    "    'series_id_column_name': 'series_id',\n",
    "    'time_column_name': 'step',\n",
    "    'event_column_name': 'event',\n",
    "    'score_column_name': 'model1_score',\n",
    "}\n",
    "\n",
    "try:\n",
    "    ap_score = score(solution, submission, tolerances, **column_names)\n",
    "    print(f\"Average Precision Score: {ap_score}\")\n",
    "except ParticipantVisibleError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd16ccf-134e-4390-8dea-ea7246ac0b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
