{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf45e631-67c5-4f01-a704-326126889246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5db770-ef45-482e-ae5d-858daa62a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"processed/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2be55b2-b283-4ed7-a2fe-b7e9dbce076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23060846"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8d11eb-736d-4869-aefe-d784bcb388a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    # === 1. Convert timestamp to datetime and remove timezone ===\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n",
    "    \n",
    "    # === 2. Hour-based features ===\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype('int8')\n",
    "    df[\"minute\"] = df[\"timestamp\"].dt.minute.astype('int8')\n",
    "    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek.astype('int8')\n",
    "    df[\"elapsed_time_from_midnight\"] = (df[\"hour\"] * 60 + df[\"minute\"]).astype('int32')\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype('int8')\n",
    "    df[\"is_night\"] = ((df[\"hour\"] >= 22) | (df[\"hour\"] <= 6)).astype('int8')\n",
    "\n",
    "    # === 3. Time-based features ===\n",
    "    periods = 20\n",
    "    df[\"anglez\"] = abs(df[\"anglez\"])\n",
    "    df[\"anglez_diff\"] = df.groupby('series_id')['anglez'].diff(periods=periods).bfill().astype('float16')\n",
    "    df[\"enmo_diff\"] = df.groupby('series_id')['enmo'].diff(periods=periods).bfill().astype('float16')\n",
    "\n",
    "    # === 4. Rolling statistical features (mean, max, std, etc.) ===\n",
    "    window_sizes = [12, 100, 360]  # in 5-second steps\n",
    "    for window in window_sizes:\n",
    "        for col in ['anglez', 'enmo']:\n",
    "            df[f'{col}_mean_{window}s'] = df[col].rolling(window, min_periods=1).mean().astype('float16')\n",
    "            df[f'{col}_std_{window}s'] = df[col].rolling(window, min_periods=1).std().astype('float16')\n",
    "            df[f'{col}_min_{window}s'] = df[col].rolling(window, min_periods=1).min().astype('float16')\n",
    "            df[f'{col}_max_{window}s'] = df[col].rolling(window, min_periods=1).max().astype('float16')\n",
    "            df[f'{col}_median_{window}s'] = df[col].rolling(window, min_periods=1).median().astype('float16')\n",
    "            df[f'{col}_cumulative_{window}s'] = df[col].rolling(window, min_periods=1).sum().astype('float16')\n",
    "\n",
    "    # === 5. Advanced Feature Engineering ===\n",
    "    df[\"anglez_delta\"] = (df[\"anglez\"] - df[\"anglez\"].shift(1)).astype('float16')\n",
    "\n",
    "    # Simple Moving Average (SMA) for enmo\n",
    "    sma_windows = [12, 100, 360]\n",
    "    for window in sma_windows:\n",
    "        df[f'enmo_sma_{window}s'] = df['enmo'].rolling(window, min_periods=1).mean().astype('float16')\n",
    "\n",
    "    # === 6. Lag Features ===\n",
    "    lag_targets = [\n",
    "        \"anglez\", \"enmo\",\n",
    "        \"anglez_mean_12s\", \"anglez_std_12s\",\n",
    "        \"enmo_mean_12s\", \"enmo_std_12s\",\n",
    "        \"anglez_mean_60s\", \"anglez_std_60s\",\n",
    "        \"enmo_mean_60s\", \"enmo_std_60s\"\n",
    "    ]\n",
    "    lag_steps = [1, 2, 3]\n",
    "    for col in lag_targets:\n",
    "        for lag in lag_steps:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag).astype('float16')\n",
    "\n",
    "    # === 7. Handle NaN values (filling missing values where appropriate) ===\n",
    "    # Separate numeric columns for NaN filling\n",
    "    # === 7. Handle NaN values (filling missing values where appropriate) ===\n",
    "    # Fill only numeric columns, converting float16 to float32 for compatibility\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].astype('float32').bfill().ffill().astype(df[numeric_cols].dtypes.to_dict())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28025a8-a4a5-47f8-9757-0d10e2e5e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"hour\",\n",
    "    \"anglez\",\n",
    "    \"anglez_mean_100s\",\n",
    "    \"anglez_max_100s\",\n",
    "    \"anglez_std_100s\",\n",
    "    \"anglez_diff\",\n",
    "    \"enmo\",\n",
    "    \"enmo_mean_100s\",\n",
    "    \"enmo_max_100s\",\n",
    "    \"enmo_std_100s\",\n",
    "    \"enmo_diff\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b15634-d905-4c23-b9b8-a8edc89aa8e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'anglez_mean_60s'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'anglez_mean_60s'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mmake_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m train[features]\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawake\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mmake_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m lag_targets:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m lag_steps:\n\u001b[0;32m---> 49\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshift(lag)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# === 7. Handle NaN values (filling missing values where appropriate) ===\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Separate numeric columns for NaN filling\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# === 7. Handle NaN values (filling missing values where appropriate) ===\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Fill only numeric columns, converting float16 to float32 for compatibility\u001b[39;00m\n\u001b[1;32m     55\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'anglez_mean_60s'"
     ]
    }
   ],
   "source": [
    "train = make_features(train)\n",
    "\n",
    "X = train[features]\n",
    "y = train[\"awake\"]\n",
    "groups = train[\"series_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ff900-6251-4e1f-81f7-60d387d444f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_splits = 3\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "oof_preds = np.zeros(len(X))\n",
    "oof_preds_not_awake = np.zeros(len(X))\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "# Optional: speichern der Metriken pro Fold\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "    print(f\"\\n🔁 Fold {fold + 1}\")\n",
    "\n",
    "    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Modell trainieren\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        min_samples_leaf=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # Vorhersagen\n",
    "    probs = model.predict_proba(X_val)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    oof_preds[val_idx] = probs[:, 1]             # awake-Wahrscheinlichkeit\n",
    "    oof_preds_not_awake[val_idx] = probs[:, 0]   # nicht-awake\n",
    "\n",
    "    # Metriken\n",
    "    precision = precision_score(y_val, preds)\n",
    "    recall = recall_score(y_val, preds)\n",
    "    f1 = f1_score(y_val, preds)\n",
    "    ap = average_precision_score(y_val, probs[:, 1])\n",
    "\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'average_precision': ap\n",
    "    })\n",
    "\n",
    "    print(f\"📈 Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AP: {ap:.4f}\")\n",
    "\n",
    "    # Feature Importances aufsummieren\n",
    "    feature_importances += model.feature_importances_\n",
    "\n",
    "# Feature-Importance Mittelwert über Folds\n",
    "avg_importances = feature_importances / n_splits\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': avg_importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Metriken zusammenfassen\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(\"\\n📊 Fold-wise metrics:\\n\", metrics_df)\n",
    "print(f\"\\n🔍 Mean Average Precision: {metrics_df['average_precision'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n🔥 Feature Importances:\\n\", importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdf30b-3a0c-4a04-b037-5978ea0c4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Nur einmalig am Ende des letzten Folds oder außerhalb der Schleife\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns  # oder X_train.columns\n",
    "\n",
    "# In DataFrame\n",
    "feat_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Anzeige\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_df[\"feature\"], feat_df[\"importance\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Feature Importances (letzter Fold)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563fdf6-4e11-47fe-bd80-0ffcb0bbb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train[\"awake\"] = oof_preds\n",
    "train[\"not_awake\"] = oof_preds_not_awake\n",
    "\n",
    "\n",
    "# Smoothing the predictions\n",
    "smoothing_length = 2*230\n",
    "train[\"score\"]  = train[\"awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "train[\"smooth\"] = train[\"not_awake\"].rolling(smoothing_length,center=True).mean().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "\n",
    "# Re-binarize the smooth values\n",
    "train[\"smooth\"] = train[\"smooth\"].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a550d-1404-477a-9d1a-2c64e670001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to determine the onset and wakeup events\n",
    "def get_event(df):\n",
    "    lstCV = zip(df.series_id, df.smooth)\n",
    "    lstPOI = []\n",
    "    for (c, v), g in groupby(lstCV, lambda cv: (cv[0], cv[1] != 0 and not pd.isnull(cv[1]))):\n",
    "        llg = sum(1 for item in g)\n",
    "        if v is False:\n",
    "            lstPOI.extend([0] * llg)\n",
    "        else:\n",
    "            lstPOI.extend(['onset'] + (llg - 2) * [0] + ['wakeup'] if llg > 1 else [0])\n",
    "    return lstPOI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34f1c2-cc87-417e-ad59-84584e887480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa3678-0dd6-4d8f-82ee-7b6b340fa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[\"event\"] = get_event(train)\n",
    "\n",
    "# Save the events for inspection\n",
    "train_events = train.loc[train[\"event\"] != 0][[\"series_id\", \"step\", \"event\", \"score\"]].copy().reset_index(drop=True)\n",
    "train_events.to_csv('result_events.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Sleep events for training data saved under: result_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123ea45-1a10-489f-b39b-da68ecb67a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train events CSV file into a DataFrame\n",
    "train_events = pd.read_csv('result_events.csv')\n",
    "\n",
    "# Display the first few rows to check the data\n",
    "print(train_events.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373f34-203d-4ebb-8d14-1392a68908d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_events)\n",
    "\n",
    "# Set a threshold for valid pairings (step difference > 15000 is invalid)\n",
    "step_diff_min_threshold = 2000\n",
    "\n",
    "# Sort by series_id and step\n",
    "df = df.sort_values(by=['series_id', 'step'])\n",
    "\n",
    "# Initialize a list to store valid event pairs\n",
    "valid_pairs = []\n",
    "\n",
    "# Iterate through each unique series_id and match onsets and wakeups\n",
    "for series_id in df['series_id'].unique():\n",
    "    series_data = df[df['series_id'] == series_id]\n",
    "    \n",
    "    onset_event = None\n",
    "    for index, row in series_data.iterrows():\n",
    "        if row['event'] == 'onset':\n",
    "            onset_event = row\n",
    "        elif row['event'] == 'wakeup' and onset_event is not None:\n",
    "            # Ensure the step difference between onset and wakeup is above the minimum threshold\n",
    "            step_diff = abs(row['step'] - onset_event['step'])\n",
    "            if step_diff >= step_diff_min_threshold:\n",
    "                valid_pairs.append((onset_event, row))  # Add the valid pair\n",
    "            onset_event = None  # Reset for next pairing\n",
    "\n",
    "# Create a DataFrame for the valid pairs in the required format\n",
    "output_data = []\n",
    "for onset, wakeup in valid_pairs:\n",
    "    output_data.append({\n",
    "        'series_id': onset['series_id'],\n",
    "        'step': onset['step'],\n",
    "        'event': 'onset',\n",
    "        'score': onset['score']\n",
    "    })\n",
    "    output_data.append({\n",
    "        'series_id': wakeup['series_id'],\n",
    "        'step': wakeup['step'],\n",
    "        'event': 'wakeup',\n",
    "        'score': wakeup['score']\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the output data\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_df.to_csv('valid_pairs_predictions.csv', index=False)\n",
    "\n",
    "print(\"Results saved to 'valid_pairs_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1d0af-26b5-4291-a022-fc9bbaa5e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from event_detection_ap import score, ParticipantVisibleError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fdb32-8c36-46c1-a9c8-8e8529ccffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load ground truth and predictions\n",
    "solution = pd.read_csv('processed/event_cleaned_final.csv')             # Ground truth\n",
    "submission = pd.read_csv('valid_pairs_predictions.csv')                      # Your predictions from train set\n",
    "\n",
    "# Define tolerances\n",
    "tolerances = {\n",
    "    \"onset\":  [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "}\n",
    "\n",
    "# Set correct column names used in your prediction file\n",
    "column_names = {\n",
    "    'series_id_column_name': 'series_id',\n",
    "    'time_column_name': 'step',\n",
    "    'event_column_name': 'event',\n",
    "    'score_column_name': 'score',  # You named the prediction confidence 'score'\n",
    "}\n",
    "\n",
    "# Run scoring\n",
    "try:\n",
    "    ap_score = score(solution, submission, tolerances, **column_names)\n",
    "    print(f\"\\n✅ Average Precision Score: {ap_score}\")\n",
    "except ParticipantVisibleError as e:\n",
    "    print(f\"\\n❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b40bcd-8dd5-4c00-be4f-8860c6188512",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.read_csv('processed/event_cleaned_final.csv')\n",
    "submission = pd.read_csv('valid_pairs_predictions.csv')\n",
    "\n",
    "# Function to count events per series_id\n",
    "def count_events(df, label):\n",
    "    counts = df[df[\"event\"].isin([\"onset\", \"wakeup\"])] \\\n",
    "                .groupby([\"series_id\", \"event\"]) \\\n",
    "                .size() \\\n",
    "                .unstack(fill_value=0) \\\n",
    "                .reset_index()\n",
    "    counts[\"source\"] = label\n",
    "    return counts\n",
    "\n",
    "# Apply to both datasets\n",
    "solution_counts = count_events(solution, \"ground_truth\")\n",
    "submission_counts = count_events(submission, \"prediction\")\n",
    "\n",
    "# Summary\n",
    "print(f\"🔍 Ground Truth: {solution['series_id'].nunique()} series_ids\")\n",
    "print(f\"🔍 Predictions : {submission['series_id'].nunique()} series_ids\\n\")\n",
    "\n",
    "print(\"📊 Ground Truth Event Counts:\")\n",
    "print(solution_counts.head(50))\n",
    "\n",
    "print(\"\\n📊 Prediction Event Counts:\")\n",
    "print(submission_counts.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1a080-0f1a-4ee5-b226-5a2c162d5806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
