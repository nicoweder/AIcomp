{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2826930-cd2c-43f0-b5a6-4425a0d090c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from xgboost) (1.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e59f963-a253-4bb4-95e0-7b9ccdeb4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from event_detection_ap import score, ParticipantVisibleError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a615f3fb-a079-49e1-a7fd-cac1ffdd4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "N_SPLITS = 5\n",
    "\n",
    "\n",
    "# Dateipfade\n",
    "DATA_PATH = \"processed/merged_dff_gold84_V3.parquet\"\n",
    "CANDIDATES_PATH = \"results/model1_candidates.csv\"\n",
    "EVENTS_PATH = \"processed/event_cleaned.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02030b7-ace9-400c-a889-1fcadcb06ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(y_probs, threshold):\n",
    "    # Apply threshold to the probabilities\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    return np.where(y_pred == 1)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111dde1a-439e-440f-ab1c-6f99e27bcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_windowing(df):\n",
    "    df[\"step_int\"] = df[\"step\"].astype(int)\n",
    "    df[\"step_offset\"] = df.groupby(\"series_id\")[\"step_int\"].transform(lambda x: x - x.min())\n",
    "    return df[df[\"step_offset\"] % 3 == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b94b42-7847-45fe-843c-491ed21fd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_labels(df, radius=4):\n",
    "    def expand_group(group):\n",
    "        target_array = group[\"target\"].values.copy()\n",
    "        shifted_targets = {}\n",
    "        for shift in range(1, radius + 1):\n",
    "            shifted_targets[f\"minus_{shift}\"] = group[\"target\"].shift(-shift, fill_value=0).values\n",
    "            shifted_targets[f\"plus_{shift}\"] = group[\"target\"].shift(shift, fill_value=0).values\n",
    "\n",
    "        for values in shifted_targets.values():\n",
    "            target_array |= values\n",
    "\n",
    "        group = group.copy()  \n",
    "        group[\"target\"] = target_array\n",
    "        return group\n",
    "\n",
    "    df = df.groupby(\"series_id\", group_keys=False).apply(expand_group)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64eb9aee-5abf-4c43-94aa-8ab6d4c5a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48dfcb03-ac16-4fcd-870a-47e038eadad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Set\n",
    "final_preset = [\n",
    "    \"anglez\", \"enmo\", \"hour\", \"minute\", \"is_night\", \"is_weekend\",\n",
    "    \"anglez_delta\", \"anglez_lag_1\", \"enmo_lag_1\",\n",
    "    \"anglez_mean_60s\", \"enmo_mean_60s\", \"enmo_std_60s\",\n",
    "    \"anglez_min_60s\", \"enmo_max_60s\",\n",
    "    \"enmo_cumulative_60s\", \"enmo_sma_60s\",\n",
    "    \"enmo_mean_12s_lag_1\", \"anglez_std_60s_lag_1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bafd32-7c32-4424-95c7-29ba16d28851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "missing_features = [feat for feat in final_preset if feat not in df.columns]\n",
    "print(missing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d12013-04e0-4eea-b6de-84a0f83c2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment_simple(\n",
    "    feature_set,\n",
    "    radius=6,\n",
    "    threshold=0.9,\n",
    "    model_params=None,\n",
    "    save_results=True\n",
    "):\n",
    "    df_exp = candidate_windowing(df)\n",
    "    df_exp = expand_labels(df_exp, radius=radius)\n",
    "\n",
    "    X = df_exp[feature_set].astype(np.float32)\n",
    "    y = df_exp[\"target\"]\n",
    "    groups = df_exp[\"series_id\"]\n",
    "    meta = df_exp[[\"series_id\", \"step\"]].copy()\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    all_probs = []\n",
    "    all_meta = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        meta_val = meta.iloc[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**model_params)\n",
    "        weights = compute_sample_weight(\"balanced\", y_train)\n",
    "        model.fit(X_train, y_train, sample_weight=weights)\n",
    "\n",
    "        y_probs = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "        all_probs.extend(y_probs)\n",
    "        all_preds.extend(y_pred)\n",
    "        all_true.extend(y_val)\n",
    "        all_meta.append(meta_val)\n",
    "\n",
    "    precision = precision_score(all_true, all_preds)\n",
    "    recall = recall_score(all_true, all_preds)\n",
    "    f1 = f1_score(all_true, all_preds)\n",
    "    print(X_train.dtypes)\n",
    "    \n",
    "    if save_results:\n",
    "        results_df = pd.concat(all_meta).copy()\n",
    "        results_df[\"true_label\"] = all_true\n",
    "        results_df[\"pred_label\"] = all_preds\n",
    "        results_df[\"score\"] = all_probs\n",
    "\n",
    "        \n",
    "        filename = f\"results/newmodel_preds.csv\"\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved predictions to: {filename}\")\n",
    "    \n",
    "    import gc\n",
    "\n",
    "    # After model training:\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febce634-aa32-4d7e-86db-ca739c792e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for feature set: base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11105/749779419.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"series_id\", group_keys=False).apply(expand_group)\n",
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:10:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:10:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:11:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:12:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [13:12:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anglez                  float32\n",
      "enmo                    float32\n",
      "hour                    float32\n",
      "minute                  float32\n",
      "is_night                float32\n",
      "is_weekend              float32\n",
      "anglez_delta            float32\n",
      "anglez_lag_1            float32\n",
      "enmo_lag_1              float32\n",
      "anglez_mean_60s         float32\n",
      "enmo_mean_60s           float32\n",
      "enmo_std_60s            float32\n",
      "anglez_min_60s          float32\n",
      "enmo_max_60s            float32\n",
      "enmo_cumulative_60s     float32\n",
      "enmo_sma_60s            float32\n",
      "enmo_mean_12s_lag_1     float32\n",
      "anglez_std_60s_lag_1    float32\n",
      "dtype: object\n",
      "Saved predictions to: results/newmodel_preds.csv\n",
      "Experiment Results for Radius=50, Threshold=0.9: Precision=0.101, Recall= 0.862, F1=0.180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Fixed set of features\n",
    "feature_sets = {\n",
    "    \"base\": final_preset  \n",
    "}\n",
    "\n",
    "radius = 50\n",
    "threshold = 0.9\n",
    "\n",
    "# Model best hyperparameters (for now)\n",
    "model_params = {\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"n_estimators\": 500,        \n",
    "                \"max_depth\": 4,              \n",
    "                \"learning_rate\": 0.02,       \n",
    "                \"eval_metric\": \"logloss\",\n",
    "                 \"tree_method\": \"hist\",\n",
    "                  \"scale_pos_weight\": 10,  \n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"gamma\": 1,\n",
    "                 \"min_child_weight\": 5,\n",
    "                 \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Run the experiment for a fixed set of parameters\n",
    "for name, feats in feature_sets.items():\n",
    "    print(f\"Running experiment for feature set: {name}\")\n",
    "    \n",
    "    p, r_, f = run_experiment_simple(\n",
    "        feats,\n",
    "        radius=radius,\n",
    "        threshold=threshold,\n",
    "        model_params=model_params\n",
    "    )\n",
    "\n",
    "    print(f\"Experiment Results for Radius={radius}, Threshold={threshold}: \"\n",
    "          f\"Precision={p:.3f}, Recall={r_: .3f}, F1={f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd16ccf-134e-4390-8dea-ea7246ac0b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved as 'newmodel_preds_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CSV\n",
    "dfn = pd.read_csv(\"results/newmodel_preds.csv\")\n",
    "\n",
    "# Step 2: Keep only rows where pred_label == 1 (the candidates)\n",
    "dfn = dfn[dfn[\"pred_label\"] == 1]\n",
    "\n",
    "# Step 3: Drop the 'true_label' column\n",
    "dfn = dfn.drop(columns=[\"true_label\", \"pred_label\"])\n",
    "\n",
    "# Step 4 (Optional): Save the cleaned CSV\n",
    "dfn.to_csv(\"results/newmodel_preds_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaned file saved as 'newmodel_preds_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d832b4c-40d0-44d2-b0ef-a367db0f0f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 0 - 100000\n",
      "✅ Processed batch 100000 - 200000\n",
      "✅ Processed batch 200000 - 300000\n",
      "✅ Processed batch 300000 - 400000\n",
      "✅ Processed batch 400000 - 500000\n",
      "✅ Processed batch 500000 - 600000\n",
      "✅ Processed batch 600000 - 700000\n",
      "✅ Processed batch 700000 - 800000\n",
      "✅ Processed batch 800000 - 900000\n",
      "✅ Processed batch 900000 - 1000000\n",
      "✅ Processed batch 1000000 - 1100000\n",
      "✅ Processed batch 1100000 - 1200000\n",
      "✅ Processed batch 1200000 - 1300000\n",
      "✅ Processed batch 1300000 - 1400000\n",
      "✅ Processed batch 1400000 - 1500000\n",
      "✅ Processed batch 1500000 - 1600000\n",
      "✅ Processed batch 1600000 - 1700000\n",
      "✅ Processed batch 1700000 - 1800000\n",
      "✅ Processed batch 1800000 - 1900000\n",
      "✅ Processed batch 1900000 - 2000000\n",
      "✅ Processed batch 2000000 - 2100000\n",
      "✅ Processed batch 2100000 - 2200000\n",
      "✅ Processed batch 2200000 - 2300000\n",
      "✅ Processed batch 2300000 - 2400000\n",
      "✅ Processed batch 2400000 - 2500000\n",
      "✅ Processed batch 2500000 - 2600000\n",
      "✅ Final merged DataFrame shape: (2518917, 21)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100_000  # you can adjust this depending on your memory\n",
    "merged_batches = []\n",
    "\n",
    "# Load your main dataframe only once\n",
    "df_features = df[['series_id', 'step'] + final_preset]\n",
    "\n",
    "# Process in batches\n",
    "for start_idx in range(0, len(dfn), batch_size):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = dfn.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Merge the batch\n",
    "    merged_batch = pd.merge(batch, df_features, how='left', on=['series_id', 'step'])\n",
    "    merged_batches.append(merged_batch)\n",
    "\n",
    "    print(f\"✅ Processed batch {start_idx} - {end_idx}\")\n",
    "\n",
    "# Combine all merged batches into one big DataFrame\n",
    "merged_df = pd.concat(merged_batches, ignore_index=True)\n",
    "\n",
    "# Save\n",
    "merged_df.to_csv(CANDIDATES_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Final merged DataFrame shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7794f-8835-4ddf-97bd-7b8ded0c64e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
