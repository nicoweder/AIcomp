{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf45e631-67c5-4f01-a704-326126889246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from event_detection_ap import score, ParticipantVisibleError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5db770-ef45-482e-ae5d-858daa62a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"processed/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2be55b2-b283-4ed7-a2fe-b7e9dbce076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23060846"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8d11eb-736d-4869-aefe-d784bcb388a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    print(\"üîÑ Starting feature generation...\")\n",
    "\n",
    "    # === 1. Convert timestamp to datetime and remove timezone ===\n",
    "    print(\"üïí Converting timestamp to datetime...\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n",
    "\n",
    "    # === 2. Hour-based features ===\n",
    "    print(\"‚è∞ Generating time-based features...\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype('int8')\n",
    "    df[\"minute\"] = df[\"timestamp\"].dt.minute.astype('int8')\n",
    "    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek.astype('int8')\n",
    "    df[\"elapsed_time_from_midnight\"] = (df[\"hour\"] * 60 + df[\"minute\"]).astype('int32')\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype('int8')\n",
    "    df[\"is_night\"] = ((df[\"hour\"] >= 22) | (df[\"hour\"] <= 6)).astype('int8')\n",
    "\n",
    "    # === 3. Time-based features ===\n",
    "    print(\"üìà Calculating time-differential features...\")\n",
    "    periods = 20\n",
    "    df[\"anglez\"] = abs(df[\"anglez\"])\n",
    "    df[\"anglez_diff\"] = df.groupby('series_id')['anglez'].diff(periods=periods).bfill().astype('float16')\n",
    "    df[\"enmo_diff\"] = df.groupby('series_id')['enmo'].diff(periods=periods).bfill().astype('float16')\n",
    "\n",
    "    # === 4. Rolling statistical features (mean, max, std, etc.) ===\n",
    "    print(\"üîÅ Computing rolling statistics...\")\n",
    "    window_sizes = [12, 100, 360]\n",
    "    for window in window_sizes:\n",
    "        print(f\"  ‚û§ Rolling features for window size: {window}\")\n",
    "        for col in ['anglez', 'enmo']:\n",
    "            df[f'{col}_mean_{window}s'] = df[col].rolling(window, min_periods=1).mean().astype('float16')\n",
    "            df[f'{col}_std_{window}s'] = df[col].rolling(window, min_periods=1).std().astype('float16')\n",
    "            df[f'{col}_min_{window}s'] = df[col].rolling(window, min_periods=1).min().astype('float16')\n",
    "            df[f'{col}_max_{window}s'] = df[col].rolling(window, min_periods=1).max().astype('float16')\n",
    "            df[f'{col}_median_{window}s'] = df[col].rolling(window, min_periods=1).median().astype('float16')\n",
    "            df[f'{col}_cumulative_{window}s'] = df[col].rolling(window, min_periods=1).sum().astype('float16')\n",
    "\n",
    "\n",
    "    # === 6. Lag Features ===\n",
    "    print(\"‚è™ Adding lag features...\")\n",
    "    lag_targets = [\n",
    "        \"anglez\", \"enmo\",\n",
    "        \"anglez_mean_12s\", \"anglez_std_12s\",\n",
    "        \"enmo_mean_12s\", \"enmo_std_12s\",\n",
    "        \"anglez_mean_100s\", \"anglez_std_100s\",\n",
    "        \"enmo_mean_100s\", \"enmo_std_100s\"\n",
    "    ]\n",
    "    lag_steps = [1, 2, 3]\n",
    "    for col in lag_targets:\n",
    "        for lag in lag_steps:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag).astype('float16')\n",
    "\n",
    "    # === 7. Handle NaN values ===\n",
    "    print(\"üßπ Filling missing values...\")\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].astype('float32').bfill().ffill().astype(df[numeric_cols].dtypes.to_dict())\n",
    "\n",
    "    print(\"‚úÖ Feature generation complete. Total features:\", len(df.columns))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28025a8-a4a5-47f8-9757-0d10e2e5e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [\n",
    "    \"hour\", \"anglez_mean_100s\", \"anglez_std_100s\", \"anglez_diff\",\n",
    "    \"enmo\", \"enmo_mean_100s\", \"enmo_max_100s\", \"enmo_std_100s\", \"enmo_diff\"\n",
    "]\n",
    "\n",
    "extended_features1 = base_features + [\n",
    "    \"anglez_mean_360s\", \"anglez_std_360s\",\n",
    "    \"enmo_std_360s\", \"is_night\", \"is_weekend\"\n",
    "]\n",
    "\n",
    "extended_features2 = base_features + [\n",
    "    \n",
    "    # Lag features\n",
    "    \"anglez_std_100s_lag_3\",\n",
    "    \"anglez_std_100s_lag_2\",\n",
    "    \"anglez_std_100s_lag_1\",\n",
    "    \"enmo_mean_100s_lag_1\",\n",
    "    \"enmo_mean_100s_lag_2\",\n",
    "    \"enmo_mean_100s_lag_3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc4fb2a-fa9a-461d-86b4-10f075880147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting feature generation...\n",
      "üïí Converting timestamp to datetime...\n",
      "‚è∞ Generating time-based features...\n",
      "üìà Calculating time-differential features...\n",
      "üîÅ Computing rolling statistics...\n",
      "  ‚û§ Rolling features for window size: 12\n",
      "  ‚û§ Rolling features for window size: 100\n",
      "  ‚û§ Rolling features for window size: 360\n",
      "‚è™ Adding lag features...\n",
      "üßπ Filling missing values...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mmake_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mmake_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müßπ Filling missing values...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 55\u001b[0m df[numeric_cols] \u001b[38;5;241m=\u001b[39m df[numeric_cols]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mbfill()\u001b[38;5;241m.\u001b[39mffill()\u001b[38;5;241m.\u001b[39mastype(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Feature generation complete. Total features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[1;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[1;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[1;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[1;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[1;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4131\u001b[0m     )\n\u001b[0;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4140\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/array_algos/take.py:345\u001b[0m, in \u001b[0;36m_get_take_nd_function.<locals>.func\u001b[0;34m(arr, indexer, out, fill_value)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(arr, indexer, out, fill_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m--> 345\u001b[0m     \u001b[43m_take_nd_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_info\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/array_algos/take.py:528\u001b[0m, in \u001b[0;36m_take_nd_object\u001b[0;34m(arr, indexer, out, axis, fill_value, mask_info)\u001b[0m\n\u001b[1;32m    526\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(out\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape[axis] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 528\u001b[0m     \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_masking:\n\u001b[1;32m    530\u001b[0m     outindexer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = make_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe691e12-2d9a-4f06-9651-34e5ad0508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showcor(X):\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(X.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "    plt.title(\"Feature Correlation Heatmap (with values)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ff900-6251-4e1f-81f7-60d387d444f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_random_forest(train_df, features, n_splits=3, model_params=None):\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'n_estimators': 100,\n",
    "            'min_samples_leaf': 300,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "    X = train_df[features]\n",
    "    y = train_df[\"awake\"]\n",
    "    groups = train_df[\"series_id\"]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    oof_preds_not_awake = np.zeros(len(X))\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "        print(f\"\\nüîÅ Fold {fold + 1}\")\n",
    "        print(f\"  ‚û§ Training size: {len(train_idx)} | Validation size: {len(val_idx)}\")\n",
    "\n",
    "        X_tr, y_tr = X.iloc[train_idx].copy(), y.iloc[train_idx].copy()\n",
    "        X_val, y_val = X.iloc[val_idx].copy(), y.iloc[val_idx].copy()\n",
    "\n",
    "        model = RandomForestClassifier(**model_params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        probs = model.predict_proba(X_val)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        oof_preds[val_idx] = probs[:, 1]\n",
    "        oof_preds_not_awake[val_idx] = probs[:, 0]\n",
    "\n",
    "        precision = precision_score(y_val, preds)\n",
    "        recall = recall_score(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds)\n",
    "\n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "\n",
    "        print(f\"  üìà Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "        feature_importances += model.feature_importances_\n",
    "\n",
    "    avg_importances = feature_importances / n_splits\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': avg_importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    metrics_df = pd.DataFrame(fold_metrics)\n",
    "    print(\"\\nüìä Fold-wise metrics:\\n\", metrics_df)\n",
    "    print(f\"\\nüîç Mean F1 Score: {metrics_df['f1_score'].mean():.4f}\")\n",
    "\n",
    "    print(\"\\nüî• Top 10 Feature Importances:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    return oof_preds, oof_preds_not_awake, model, X.columns, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563fdf6-4e11-47fe-bd80-0ffcb0bbb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_predictions(df, awake_col=\"awake_pred\", not_awake_col=\"not_awake_pred\", smoothing_length=460):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"score\"] = df[awake_col].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "    df[\"smooth\"] = df[not_awake_col].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "    \n",
    "    df[\"smooth\"] = df[\"smooth\"].round()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a550d-1404-477a-9d1a-2c64e670001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to determine the onset and wakeup events\n",
    "def get_event(df):\n",
    "    lstCV = zip(df.series_id, df.smooth)\n",
    "    lstPOI = []\n",
    "    for (c, v), g in groupby(lstCV, lambda cv: (cv[0], cv[1] != 0 and not pd.isnull(cv[1]))):\n",
    "        llg = sum(1 for item in g)\n",
    "        if v is False:\n",
    "            lstPOI.extend([0] * llg)\n",
    "        else:\n",
    "            lstPOI.extend(['onset'] + (llg - 2) * [0] + ['wakeup'] if llg > 1 else [0])\n",
    "    return lstPOI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373f34-203d-4ebb-8d14-1392a68908d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_filter(result_events, step_diff_min):\n",
    "    df = pd.DataFrame(result_events)\n",
    "    \n",
    "    # Set a threshold for valid pairings (step difference > 15000 is invalid)\n",
    "    step_diff_min_threshold = step_diff_min\n",
    "    \n",
    "    # Sort by series_id and step\n",
    "    df = df.sort_values(by=['series_id', 'step'])\n",
    "    \n",
    "    # Initialize a list to store valid event pairs\n",
    "    valid_pairs = []\n",
    "    \n",
    "    # Iterate through each unique series_id and match onsets and wakeups\n",
    "    for series_id in df['series_id'].unique():\n",
    "        series_data = df[df['series_id'] == series_id]\n",
    "        \n",
    "        onset_event = None\n",
    "        for index, row in series_data.iterrows():\n",
    "            if row['event'] == 'onset':\n",
    "                onset_event = row\n",
    "            elif row['event'] == 'wakeup' and onset_event is not None:\n",
    "                # Ensure the step difference between onset and wakeup is above the minimum threshold\n",
    "                step_diff = abs(row['step'] - onset_event['step'])\n",
    "                if step_diff >= step_diff_min_threshold:\n",
    "                    valid_pairs.append((onset_event, row))  # Add the valid pair\n",
    "                onset_event = None  # Reset for next pairing\n",
    "    \n",
    "    # Create a DataFrame for the valid pairs in the required format\n",
    "    output_data = []\n",
    "    for onset, wakeup in valid_pairs:\n",
    "        output_data.append({\n",
    "            'series_id': onset['series_id'],\n",
    "            'step': onset['step'],\n",
    "            'event': 'onset',\n",
    "            'score': onset['score']\n",
    "        })\n",
    "        output_data.append({\n",
    "            'series_id': wakeup['series_id'],\n",
    "            'step': wakeup['step'],\n",
    "            'event': 'wakeup',\n",
    "            'score': wakeup['score']\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the output data\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    output_df.to_csv('valid_pairs_predictions.csv', index=False)\n",
    "    \n",
    "    print(\"Results saved to 'valid_pairs_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fdb32-8c36-46c1-a9c8-8e8529ccffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_pipeline():\n",
    "\n",
    "\n",
    "    # Load ground truth and predictions\n",
    "    solution = pd.read_csv('processed/event_cleaned_final.csv')             # Ground truth\n",
    "    submission = pd.read_csv('valid_pairs_predictions.csv')                      # Your predictions from train set\n",
    "    \n",
    "    # Define tolerances\n",
    "    tolerances = {\n",
    "        \"onset\":  [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "        \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    }\n",
    "    \n",
    "    # Set correct column names used in your prediction file\n",
    "    column_names = {\n",
    "        'series_id_column_name': 'series_id',\n",
    "        'time_column_name': 'step',\n",
    "        'event_column_name': 'event',\n",
    "        'score_column_name': 'score',  # You named the prediction confidence 'score'\n",
    "    }\n",
    "    \n",
    "    # Run scoring\n",
    "    try:\n",
    "        ap_score = score(solution, submission, tolerances, **column_names)\n",
    "        print(f\"\\n‚úÖ Average Precision Score: {ap_score}\")\n",
    "    except ParticipantVisibleError as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "\n",
    "\n",
    "    return ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1a080-0f1a-4ee5-b226-5a2c162d5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names, top_n=None, save_path=None, title=\"Feature Importances\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    feat_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    }).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    if top_n:\n",
    "        feat_df = feat_df.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_df[\"feature\"], feat_df[\"importance\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_path:\n",
    "        feat_df.to_csv(save_path, index=False)\n",
    "        print(f\"üìÅ Feature importances saved to: {save_path}\")\n",
    "    \n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7ff6d-1516-4795-b72d-5bc59356a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(train, features, step_diff_min=2000, smoothing_length=460, n_splits=3, model_params=None):\n",
    "    print(\"üöÄ Starting full pipeline...\")\n",
    "\n",
    "    train_df = train\n",
    "\n",
    "    oof_preds, oof_preds_not_awake, model, cols, importances = run_cv_random_forest(\n",
    "        train_df, features, n_splits=n_splits, model_params=model_params\n",
    "    )\n",
    "    plot_feature_importances(model, cols, top_n=15, save_path=\"feature_importances.csv\")\n",
    "\n",
    "    train_df[\"awake_pred\"] = oof_preds\n",
    "    train_df[\"not_awake_pred\"] = oof_preds_not_awake\n",
    "\n",
    "    # üëá Pass smoothing_length here\n",
    "    train_df = smooth_predictions(train_df, smoothing_length=smoothing_length)\n",
    "\n",
    "    train_df[\"event\"] = get_event(train_df)\n",
    "    train_events = train_df.loc[train_df[\"event\"] != 0, [\"series_id\", \"step\", \"event\", \"score\"]].reset_index(drop=True)\n",
    "    train_events.to_csv('result_events.csv', index=False)\n",
    "    print(\"\\n‚úÖ Events saved to: result_events.csv\")\n",
    "\n",
    "    apply_filter(train_events, step_diff_min=step_diff_min)\n",
    "    ap_score = evaluate_full_pipeline()\n",
    "\n",
    "    return ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d10366-7104-4b00-82f2-4387d51122ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633aab2-d385-42ac-b2dc-85aade53e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "\n",
    "def parameter_sweep_with_features(\n",
    "    train,\n",
    "    feature_sets,\n",
    "    param_grid,\n",
    "    step_diff_min_values=[2000],\n",
    "    smoothing_lengths=[460],\n",
    "    n_splits=3,\n",
    "    result_csv_path=\"sweep_results.csv\"\n",
    "):\n",
    "    results = []\n",
    "    default_params = {\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    # Initialize CSV file with header if it doesn't exist\n",
    "    try:\n",
    "        existing_df = pd.read_csv(result_csv_path)\n",
    "        print(f\"üìÑ Appending to existing result file: {result_csv_path}\")\n",
    "    except FileNotFoundError:\n",
    "        pd.DataFrame(columns=[\n",
    "            'timestamp', 'feature_set', 'features', 'params',\n",
    "            'step_diff_min', 'smoothing_length', 'ap_score'\n",
    "        ]).to_csv(result_csv_path, index=False)\n",
    "        print(f\"üìÑ Created new result file: {result_csv_path}\")\n",
    "\n",
    "    for feature_set_name, feature_list in feature_sets.items():\n",
    "        print(f\"\\nüß™ Testing feature set: {feature_set_name}\")\n",
    "        for step_diff_min in step_diff_min_values:\n",
    "            for smoothing_length in smoothing_lengths:\n",
    "                for comb in product(*values):\n",
    "                    sweep_params = dict(zip(keys, comb))\n",
    "                    model_params = {**default_params, **sweep_params}\n",
    "\n",
    "                    print(f\"\\nüîç Running with smoothing={smoothing_length}, step_diff_min={step_diff_min}, params={model_params}\")\n",
    "\n",
    "                    try:\n",
    "                        ap_score = main_pipeline(\n",
    "                            train=train,\n",
    "                            features=feature_list,\n",
    "                            step_diff_min=step_diff_min,\n",
    "                            smoothing_length=smoothing_length,\n",
    "                            n_splits=n_splits,\n",
    "                            model_params=model_params\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Failed: {e}\")\n",
    "                        ap_score = None\n",
    "\n",
    "                    result = {\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'feature_set': feature_set_name,\n",
    "                        'features': ', '.join(feature_list),\n",
    "                        'params': model_params,\n",
    "                        'step_diff_min': step_diff_min,\n",
    "                        'smoothing_length': smoothing_length,\n",
    "                        'ap_score': ap_score\n",
    "                    }\n",
    "\n",
    "                    # Append to results list\n",
    "                    results.append(result)\n",
    "\n",
    "                    # Write result row to CSV immediately\n",
    "                    pd.DataFrame([result]).to_csv(result_csv_path, mode='a', index=False, header=False)\n",
    "                    print(f\"üì§ Saved result to {result_csv_path}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by='ap_score', ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ddd9f-3b08-4155-9274-99fa6b1a5664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'min_samples_leaf': [300]\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    \n",
    "    \"extended1\": extended_features1,\n",
    "    \"extended2\": extended_features2,\n",
    "    \n",
    "}\n",
    "\n",
    "sweep_results = parameter_sweep_with_features(\n",
    "    train=train,\n",
    "    feature_sets=feature_sets,\n",
    "    param_grid=param_grid,\n",
    "    step_diff_min_values=[1500, 2000, 2500],\n",
    "    smoothing_lengths=[300, 460, 600],\n",
    "    result_csv_path=\"sweep_results.csv\"\n",
    ")\n",
    "\n",
    "print(sweep_results[['feature_set', 'smoothing_length', 'step_diff_min', 'params', 'ap_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15939d51-0b39-42cf-8ce3-09df9c92df1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1443969-3f9c-4c55-9eba-4f271fd6d9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
